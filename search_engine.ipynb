{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb982e2",
   "metadata": {},
   "source": [
    "# Internal Search Engine\n",
    "\n",
    "## Version\n",
    "\n",
    "**v1.0.0**  \n",
    "Initial implementation of an internal site crawler, vector-based search engine, and Google-style UI.\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "This project implements a local search engine for any website starting from a given root URL.  \n",
    "\n",
    "This is group project for Advanced Programming course by Sándor Juhász and Ádám Balázs Csapó, Spring 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Authors\n",
    "\n",
    "- Shayan Ghiaseddin  \n",
    "  MSc Business Informatics – Corvinus University of Budapest\n",
    "\n",
    "- Péter Orosz\n",
    "  BSc Data Science – Corvinus University of Budapest\n",
    "\n",
    "- Bence Balázs Balás\n",
    "  MSc Business Informatics – Corvinus University of Budapest\n",
    "\n",
    "---\n",
    "\n",
    "## License\n",
    "\n",
    "This project is released under the **MIT License**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd3556-4fa1-4d77-a66b-c19e8bb6bcd7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Package dependencies\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install numpy\n",
    "%pip install networkx\n",
    "%pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12685664",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "ROOT_URL = \"https://www.uni-corvinus.hu/\"\n",
    "MARKERS = [\"data-elementor-type|wp-page\", \"data-elementor-type|wp-post\"]\n",
    "REQUEST_LIMIT = 10\n",
    "DURATION_LIMIT = 500 # Seconds\n",
    "INTERVAL = 0.1 # Seconds\n",
    "TIMEOUT = 3 # Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1ecb73be-4065-4805-8180-02230f5491c3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#BUILD\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import numpy as np\n",
    "\n",
    "class Crawling:\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_url: str, \n",
    "            content_markup_array: list[str], \n",
    "            request_count_limit: int, \n",
    "            request_duration_limit: int,\n",
    "            request_interval: int,\n",
    "            request_timeout: int\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the indexing system with the root of the target website and the content markers.\n",
    "\n",
    "        Args:\n",
    "            root_url (str): The root domain to crawl, e.g., \"https://example.com\".\n",
    "            content_markup_array (list[str]): List of CSS selectors (e.g., \"#main\", \".content\") used to extract meaningful content.\n",
    "            request_count_limit (int): Limit for total number of requests\n",
    "            request_duration_limit (int): Limit for total duration of requesting operation (unit: seconds)\n",
    "            request_interval (int): Time interval between requests to avoid blocking by firewall\n",
    "            request_timeout (int): Request timeout in seconds\n",
    "        \"\"\"\n",
    "        self.root_url = root_url.rstrip('/')\n",
    "        self.content_markup_array = content_markup_array\n",
    "        self.request_count_limit = request_count_limit\n",
    "        self.request_duration_limit = request_duration_limit\n",
    "        self.request_interval = request_interval\n",
    "        self.request_timeout = request_timeout\n",
    "        self.visited = set()  # Set to track visited URLs\n",
    "        self.queue = [self.root_url]  # Queue initialized with root\n",
    "        self.status = (\"\", 0, 0)  # (last_url, last_status_code, last_content_length)\n",
    "\n",
    "        # Parse the root domain info to compare with internal links\n",
    "        parsed_root = urlparse(self.root_url)\n",
    "        self.root_netloc = parsed_root.netloc\n",
    "        self.root_scheme = parsed_root.scheme\n",
    "\n",
    "\n",
    "    def handler(self):\n",
    "        \"\"\"\n",
    "        Main control loop for managing URL requests and the indexing process.\n",
    "    \n",
    "        Workflow:\n",
    "            - Maintains the limits of the indexing program\n",
    "            - Maintains a queue of URLs to visit.\n",
    "            - Skips URLs already visited by checking webgraph and checking the queue too\n",
    "            - Call webgraph to insert or update nodes\n",
    "            - Trigger pagerank() to calculate the PageRank scores\n",
    "            - Call webgraph to store PageRank scores.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        request_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while len(self.queue) > 0: # Request's queue loop\n",
    "            # Check if we've reached any limits\n",
    "            if request_count >= self.request_count_limit:\n",
    "                print(\"[Crawling] Request count limit reached.\")\n",
    "                break\n",
    "            if (time.time() - start_time) > self.request_duration_limit:\n",
    "                print(\"[Crawling] Request duration limit reached.\")\n",
    "                break\n",
    "\n",
    "            url = self.queue.pop(0) # FIFO logic\n",
    "\n",
    "            # Skip if already visited\n",
    "            if url in self.visited:\n",
    "                continue\n",
    "\n",
    "            # Request the page\n",
    "            status_code, title, content, links = self.request(url)\n",
    "\n",
    "            if status_code == 200 and content: # Don't need to store responses with 404, 301, 302, 5xx ...\n",
    "                self.webgraph.insert(url, title, content, links) # Insert node and edges into graph\n",
    "\n",
    "                for link in links: # Add new, unseen links to the queue\n",
    "                    if link not in self.visited and link not in self.queue:\n",
    "                        self.queue.append(link)\n",
    "            \n",
    "            self.visited.add(url) # Mark as visited\n",
    "            request_count += 1\n",
    "            self.status = (url, status_code, len(content))\n",
    "            \n",
    "        # Request's queue ended\n",
    "        \n",
    "        print(\"[Crawling] Requesting for urls and storing the content and network have completed.\")\n",
    "        \n",
    "        # Calculate PageRank scores\n",
    "        scores = self.calculate_pagerank()\n",
    "\n",
    "        #Update nodes with PageRank scores\n",
    "        self.webgraph.update_pagerank(scores)\n",
    "        print(\"[Crawling] PageRank calculation and storing in network have completed.\")\n",
    "\n",
    "\n",
    "    def request(self, url: str) -> tuple[int, str, list[str]]:\n",
    "        \"\"\"\n",
    "        Perform HTTP GET request to fetch the web page and conditionally parse its content.\n",
    "    \n",
    "        Args:\n",
    "            url (str): The URL to request.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A structured response with the following items:\n",
    "                - 'status_code' (int): HTTP response status code.\n",
    "                - 'content' (str | None): Extracted text content from the page (using scrapper(), if status 200).\n",
    "                - 'links' (list[str]): Internal links discovered on the page (using scrapper(), if status 200).\n",
    "        \"\"\"\n",
    "        time.sleep(self.request_interval) # In case firewall has set for request rate limit\n",
    "        header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=self.request_timeout, headers=header)\n",
    "            status_code = response.status_code\n",
    "\n",
    "            if status_code == 200:\n",
    "                links, title, content = self.scrapper(response)\n",
    "                return status_code, title, content, links\n",
    "            else: # status 404, 301, 302, 500, ...\n",
    "                return status_code, \"\", \"\", []\n",
    "        except requests.RequestException:\n",
    "            # Timeout, DNS error, etc.\n",
    "            return 500, \"\", \"\", [] \n",
    "\n",
    "\n",
    "    def scrapper(self, response: requests.Response) -> tuple[list[str], str]:\n",
    "        \"\"\"\n",
    "        Extract links and main content from the HTML response.\n",
    "    \n",
    "        Args:\n",
    "            response (requests.Response): The response object containing the HTML of a web page.\n",
    "    \n",
    "        Returns:\n",
    "            tuple:\n",
    "                - list[str]: List of internal links found on the page (after filtering and resolving).\n",
    "                - str: Extracted text content from content markup selectors.\n",
    "        \"\"\"\n",
    "        try: # on some cases maybe BeautifulSoup not work well\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"[scrapper] Failed to parse HTML for URL: {response.url} — {str(e)}\")\n",
    "            return [], \"\", \"\"\n",
    "    \n",
    "        # Step 1: Extract internal links\n",
    "        links = []\n",
    "        file_extensions = ('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.txt', '.ppt', '.pptx', '.zip', '.rar', 'jpg', 'jpeg', 'png', 'webp', 'mp4', 'mov', 'gif', 'mp3', 'svg')\n",
    "\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            joined_url = urljoin(response.url, href) # in case url has expressed relativly\n",
    "            parsed_url = urlparse(joined_url)\n",
    "\n",
    "            # Filter internal links (same domain or subdomain)\n",
    "            if parsed_url.netloc.endswith(self.root_netloc):\n",
    "                # Remove URL fragment and normalize\n",
    "                clean_url = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path\n",
    "\n",
    "                # Skip files with unwanted extensions\n",
    "                if clean_url.lower().endswith(file_extensions):\n",
    "                    continue\n",
    "                links.append(clean_url)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        links = list(set(links))\n",
    "\n",
    "        # Step 2: Extract title in head \n",
    "        title = \"\"\n",
    "        if soup.find('title') is not None:\n",
    "            title = soup.find('title').get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Step 3: Extract main content \n",
    "        content = \"\"\n",
    "\n",
    "        for marker in self.content_markup_array:\n",
    "            element = None\n",
    "        \n",
    "            if marker.startswith(\"#\"):\n",
    "                element = soup.find(id=marker[1:]) # ID selector (e.g. #main)\n",
    "            elif marker.startswith(\".\"): # Class selector (e.g. .content)\n",
    "                element = soup.find(class_=marker[1:])\n",
    "            elif \"|\" in marker: # Attribute selector (e.g. data-elementor-type|wp-post)\n",
    "                attr, val = marker.split(\"|\", 1)\n",
    "                element = soup.find(attrs={attr: val})\n",
    "            else:\n",
    "                element = soup.find(marker) # Tag name (e.g. article)\n",
    "\n",
    "            if element:\n",
    "                content = element.get_text(separator=' ', strip=True)\n",
    "                if content:\n",
    "                    break\n",
    "\n",
    "        # Step 4: Fallback to <a id=\"content\"> then next sibling. This is a technique to help screen readers to go for content and skip the header text.\n",
    "        if not content:\n",
    "            anchor = soup.find('a', id='content')\n",
    "            if anchor and anchor.find_next():\n",
    "                content = anchor.find_next().get_text(separator=' ', strip=True)\n",
    "\n",
    "        # Step 5: Final fallback to <body>, the whole text in the webpage\n",
    "        if not content:\n",
    "            body = soup.find('body')\n",
    "            if body:\n",
    "                content = body.get_text(separator=' ', strip=True)\n",
    "\n",
    "        return links, title, content               \n",
    "\n",
    "\n",
    "    def calculate_pagerank(self, damping: float = 0.85, max_iter: int = 100, tol: float = 1e-6) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate the PageRank score for each node in the graph.\n",
    "\n",
    "        Args:\n",
    "            damping (float): Damping factor, usually set to 0.85.\n",
    "            max_iter (int): Maximum number of iterations.\n",
    "            tol (float): Convergence threshold.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of URL (node) to its PageRank score.\n",
    "        \"\"\"\n",
    "        nodes = list(self.webgraph.graph.nodes)\n",
    "        N = len(nodes)\n",
    "        if N == 0:\n",
    "            return {}\n",
    "\n",
    "        index = {node: i for i, node in enumerate(nodes)} # later need this to make the matrix\n",
    "        M = np.zeros((N, N))  # Transition matrix\n",
    "\n",
    "        for j, node in enumerate(nodes): # Build M (column-stochastic)\n",
    "            out_links = list(self.webgraph.graph.successors(node))\n",
    "            if out_links:\n",
    "                weight = 1.0 / len(out_links)\n",
    "                for target in out_links:\n",
    "                    if target in index:\n",
    "                        i = index[target]\n",
    "                        M[i][j] = weight\n",
    "            else:\n",
    "                M[:, j] = 1.0 / N # Distribute evenly\n",
    "\n",
    "        pr = np.ones(N) / N # Initialize PageRank vector\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            new_pr = (1 - damping) / N + damping * M @ pr\n",
    "            if np.linalg.norm(new_pr - pr, 1) < tol:\n",
    "                break\n",
    "            pr = new_pr\n",
    "\n",
    "        scores = {node: float(pr[i]) for node, i in index.items()} # Map scores back to node URLs by index\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    def get_status(self):\n",
    "        url, code, length = self.status\n",
    "        print(f\"[Crawling] Queue: {len(self.queue)} | Visited: {len(self.visited)}\")\n",
    "        print(f\"[Crawling] Last: {url} | Status: {code} | Content length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84bde1a2-8a72-4b6d-85f3-5ecfab497b0a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#BUILD\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "class WebGraph:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes an empty directed graph to store website structure.\n",
    "        Nodes are page URLs and store attributes like content and pagerank.\n",
    "        Edges represent links from one webpage to another.\n",
    "        \"\"\"\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.status = (\"\", []) # (last_inserted_url, list of up to 5 links)\n",
    "\n",
    "\n",
    "    def insert(self, url: str, title: str, content: str, links: list[str], pagerank: float = 0.0, vector: np.ndarray = None) -> None:\n",
    "        \"\"\"\n",
    "        Insert a node and its outbound links into the graph.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL of the current page.\n",
    "            content (str): Text content of the page.\n",
    "            links (list[str]): List of target URLs this page links to.\n",
    "            pagerank (float): Initial PageRank value (default 0.0).\n",
    "            vector (np.ndarray | None): Vector representation of the content.\n",
    "        \"\"\"\n",
    "        if url in self.graph.nodes: # Node exists — update content-related attributes\n",
    "            self.graph.nodes[url]['title'] = title\n",
    "            self.graph.nodes[url]['content'] = content\n",
    "            self.graph.nodes[url]['pagerank'] = pagerank\n",
    "            self.graph.nodes[url]['vector'] = vector\n",
    "        else:\n",
    "            self.graph.add_node(url, title=title, content=content, pagerank=pagerank, vector=vector)\n",
    "            \n",
    "        for link in links:\n",
    "            self.graph.add_edge(url, link)\n",
    "        \n",
    "        self.status = (url, links[:5])\n",
    "\n",
    "\n",
    "    def update_pagerank(self, scores: dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Update pagerank value of each node using calculated scores.\n",
    "\n",
    "        Args:\n",
    "            scores (dict): A dictionary mapping URL to its PageRank score.\n",
    "        \"\"\"\n",
    "        for url, score in scores.items():\n",
    "            if url in self.graph.nodes:\n",
    "                self.graph.nodes[url]['pagerank'] = score\n",
    "    \n",
    "\n",
    "    def update_vector(self, vectors: dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Update the vector representation for each node in the graph.\n",
    "\n",
    "        Args:\n",
    "            vectors (dict[str, np.ndarray]): Mapping of URL (node) to its vector representation.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for url, vector in vectors.items():\n",
    "            if url in self.graph.nodes:\n",
    "                self.graph.nodes[url]['vector'] = vector\n",
    "\n",
    "\n",
    "    def get_node(self, url: str) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieve node attributes for a given URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL of the node.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of node attributes, or empty dict if not found.\n",
    "        \"\"\"\n",
    "        return self.graph.nodes[url] if url in self.graph.nodes else {}\n",
    "    \n",
    "\n",
    "    def get_status(self):\n",
    "        url, links = self.status\n",
    "        print(f\"[WebGraph] Nodes: {self.graph.number_of_nodes()} | Edges: {self.graph.number_of_edges()}\")\n",
    "        print(f\"[WebGraph] Last inserted: {url} | Top 5 Links: {links}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "534cbb0a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#BUILD\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "class Vectorizing:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vectorizing system and prepare stopwords.\n",
    "        \"\"\"\n",
    "        self.stopwords = self.get_stopwords()\n",
    "        self.vocabulary = set()\n",
    "        self.status = (0, 0) # (vectorized_count, total_count)\n",
    "\n",
    "\n",
    "    def handler(self, contents: dict[str, str]) -> dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate vectors for all given page contents and store the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            contents (dict[str, str]): Mapping of URL to its text content.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, np.ndarray]: Mapping of URL to its vector representation.\n",
    "        \"\"\"\n",
    "        all_texts = list(contents.values())\n",
    "        self.vocabulary = self.create_vocabulary(all_texts)\n",
    "\n",
    "        vectors = {}\n",
    "        for url, content in contents.items():\n",
    "            vector = self.content_to_vector(content, self.vocabulary)\n",
    "            vectors[url] = vector\n",
    "            self.status = (len(vectors), len(contents))\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def get_stopwords(self) -> set[str]:\n",
    "        \"\"\"\n",
    "        Stores a predefined set of English stopwords.\n",
    "\n",
    "        Returns:\n",
    "            set[str]: A set of lowercase stopwords to ignore during processing.\n",
    "        \"\"\"\n",
    "        return set([\n",
    "            # English\n",
    "            \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "            \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\n",
    "            \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
    "            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "            \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "            \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\",\n",
    "            \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "            \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "            \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
    "            \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\",\n",
    "            \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
    "            \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\",\n",
    "            \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "            \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\",\n",
    "            \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
    "            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\",\n",
    "            # Hungarian\n",
    "            \"én\", \"te\", \"ő\", \"mi\", \"ti\", \"ők\", \"engem\", \"téged\", \"őt\", \"minket\", \"titeket\", \"őket\",\n",
    "            \"nekem\", \"neked\", \"neki\", \"nekünk\", \"nektek\", \"nekik\",\n",
    "            \"magam\", \"magad\", \"maga\", \"magunk\", \"magatok\", \"maguk\",\n",
    "            \"ez\", \"az\", \"ezt\", \"azt\", \"ezek\", \"azok\", \"ide\", \"oda\", \"itt\", \"ott\",\n",
    "            \"ilyen\", \"olyan\", \"mind\", \"minden\", \"semmi\", \"valami\", \"bármi\",\n",
    "            \"egy\", \"egyik\", \"másik\", \"sok\", \"kevés\", \"több\", \"kevesebb\", \"összes\",\n",
    "            \"nem\", \"sem\", \"is\", \"se\", \"már\", \"még\", \"csak\", \"most\", \"akkor\", \"aztán\",\n",
    "            \"mikor\", \"amikor\", \"ha\", \"hogy\", \"mint\", \"mert\", \"vagy\", \"és\", \"de\", \"pedig\", \"azonban\", \"bár\", \"hanem\",\n",
    "            \"lesz\", \"van\", \"volt\", \"lett\", \"lenne\", \"legyen\", \"leszek\", \"voltam\", \"leszünk\", \"vagyok\",\n",
    "            \"volna\", \"lehet\", \"kell\", \"kellett\", \"kellene\", \"akar\", \"akarok\", \"akart\",\n",
    "            \"lennék\", \"legyek\", \"légy\", \"lenni\", \"lévén\",\n",
    "            \"át\", \"alatt\", \"fölött\", \"mellett\", \"között\", \"ellen\", \"felé\", \"iránt\", \"tovább\",\n",
    "            \"előtt\", \"után\", \"óta\", \"keresztül\", \"szerint\", \"számára\", \"szemben\",\n",
    "            \"néhány\", \"egyes\", \"egyetlen\", \"más\", \"ilyen\", \"olyan\",\n",
    "            \"honnan\", \"hova\", \"hová\", \"meddig\", \"eddig\", \"hol\", \"hogy\", \"hogyan\", \"miért\",\n",
    "            \"való\", \"val\", \"nélkül\", \"által\", \"ról\", \"ről\", \"tól\", \"től\", \"ból\", \"ből\",\n",
    "            \"ban\", \"ben\", \"ra\", \"re\", \"ba\", \"be\", \"nál\", \"nél\", \"valamint\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    def to_lowercase(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert all characters in the text to lowercase.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Lowercase text.\n",
    "        \"\"\"\n",
    "        return text.lower()\n",
    "\n",
    "\n",
    "    def remove_punctuation(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove all punctuation characters from the text using \"string\" library.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text without punctuation.\n",
    "        \"\"\"\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Split the text into individual words (tokens).\n",
    "\n",
    "        Args:\n",
    "            text (str): Cleaned input text.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of words.\n",
    "        \"\"\"\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "    def remove_stop_words(self, tokenized_text: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Remove common stopwords from the tokenized text.\n",
    "\n",
    "        Args:\n",
    "            tokenized_text (list[str]): List of tokenized words.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of meaningful words.\n",
    "        \"\"\"\n",
    "        return [word for word in tokenized_text if word not in self.stopwords]\n",
    "\n",
    "\n",
    "    def create_vocabulary(self, contents: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Build a sorted vocabulary of unique, meaningful words from all content.\n",
    "\n",
    "        Args:\n",
    "            contents (list[str]): List of textual content from multiple pages.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Sorted list of unique vocabulary words.\n",
    "        \"\"\"\n",
    "        for content in contents:\n",
    "            words = self.remove_stop_words(self.tokenize(self.remove_punctuation(self.to_lowercase(content))))\n",
    "            self.vocabulary.update(words)\n",
    "        return sorted(self.vocabulary)\n",
    "\n",
    "\n",
    "    def content_to_vector(self, content: str, vocabulary: list[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert a piece of content to a bag-of-words vector based on the given vocabulary.\n",
    "\n",
    "        Args:\n",
    "            content (str): The text to vectorize.\n",
    "            vocabulary (list[str]): The vocabulary list to use as vector basis.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A numerical vector representing word counts.\n",
    "        \"\"\"\n",
    "        word_count = {word: 0 for word in vocabulary}\n",
    "        words = self.remove_stop_words(self.tokenize(self.remove_punctuation(self.to_lowercase(content))))\n",
    "        for word in words:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "        return np.array([word_count[word] for word in vocabulary])\n",
    "\n",
    "\n",
    "    def get_status(self):\n",
    "        done, total = self.status\n",
    "        print(f\"[Vectorizing] Vectorized: {done} / {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d592f4f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl...\n",
      "[Crawling] Request count limit reached.\n",
      "[Crawling] Requesting for urls and storing the content and network have completed.\n",
      "[Crawling] PageRank calculation and storing in network have completed.\n",
      "Elapsed time: 5.333587884902954\n",
      "[Crawling] Queue: 217 | Visited: 10\n",
      "[Crawling] Last: https://www.uni-corvinus.hu/post/landing-page/makerspace/ | Status: 200 | Content length: 4342\n",
      "[WebGraph] Nodes: 217 | Edges: 739\n",
      "[WebGraph] Last inserted: https://www.uni-corvinus.hu/post/landing-page/makerspace/ | Top 5 Links: ['https://www.uni-corvinus.hu/fooldal/egyetemunkrol/corvinus-doktori-iskolak/', 'https://www.uni-corvinus.hu/post/landing-page/neuro-and-digital-marketing-research-center/', 'https://www.uni-corvinus.hu/post/landing-page/corvinusoslettem/', 'https://www.uni-corvinus.hu/fooldal/kutatas/regionalis-energiagazdasagi-kutatokozpont/', 'https://www.uni-corvinus.hu/post/landing-page/makerspace/']\n",
      "\n",
      "Crawling completed.\n",
      "\n",
      "Starting vectorization...\n",
      "Elapsed time: 5.350882053375244\n",
      "\n",
      "Vectorizing complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# === Initialization ===\n",
    "start_time = time.time()\n",
    "webgraph = WebGraph()\n",
    "crawler = Crawling(ROOT_URL, MARKERS, REQUEST_LIMIT, DURATION_LIMIT, INTERVAL, TIMEOUT)\n",
    "crawler.webgraph = webgraph # The instance to store data, injected explicitly\n",
    "vectorizer = Vectorizing()\n",
    "\n",
    "# === Crawling Phase ===\n",
    "print(\"Starting crawl...\")\n",
    "crawler.handler()\n",
    "print(f\"Elapsed time: {time.time() - start_time}\")\n",
    "\n",
    "crawler.get_status()\n",
    "webgraph.get_status()\n",
    "        \n",
    "print(\"\\nCrawling completed.\\n\")\n",
    "\n",
    "# === Vectorizing Phase ===\n",
    "print(\"Starting vectorization...\")\n",
    "\n",
    "# Extract contents\n",
    "contents = {\n",
    "    url: data[\"content\"]\n",
    "    for url, data in webgraph.graph.nodes(data=True)\n",
    "    if \"content\" in data\n",
    "}\n",
    "\n",
    "# Perform vectorization and store in network\n",
    "vectors = vectorizer.handler(contents)\n",
    "webgraph.update_vector(vectors)\n",
    "print(f\"Elapsed time: {time.time() - start_time}\")\n",
    "\n",
    "print(\"\\nVectorizing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b981a1e-4d66-4213-bf2b-bfd62223bbe5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#BUILD\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class Searching:\n",
    "    def __init__(self, phrase: str, vocabulary: list[str], count_results: int = 10, pagerank_score_damping: float = 1):\n",
    "        \"\"\"\n",
    "        Initialize the search system with user phrase and global vocabulary.\n",
    "\n",
    "        Args:\n",
    "            phrase (str): The user's search input.\n",
    "            vocabulary (list[str]): The full vocabulary used for vectorizing.\n",
    "            count_results (int): Number of results needed as output.\n",
    "            pagerank_score_damping (float): The factor to decrease the impact of pagerank (recommended values between 1 to 2)\n",
    "        \"\"\"\n",
    "        self.phrase = phrase\n",
    "        self.vocabulary = vocabulary\n",
    "        self.count_results = count_results\n",
    "        self.pagerank_score_damping = pagerank_score_damping\n",
    "        self.must_words = set()\n",
    "        self.must_not_words = set()\n",
    "        self.query_vector = None\n",
    "\n",
    "\n",
    "    def handler(self, vectorizer, webgraph) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Workflow:\n",
    "            - Parse phrase\n",
    "            - Create search phrase vector\n",
    "            - Check all vectors in WebGraph using filters\n",
    "            - Compute cosine similarity\n",
    "            - Combine with pagerank\n",
    "            - Return top results, formatted\n",
    "    \n",
    "        Args:\n",
    "            vectorizer (Vectorizing): Instance of Vectorizing (used for tokenizing and vocabulary).\n",
    "            webgraph (WebGraph): Instance of WebGraph (contains all nodes and their vectors).\n",
    "    \n",
    "        Returns:\n",
    "            list[dict]: List of top results with 'url', 'title', 'similarity_score', 'pagerank_score', and 'subtitle'.\n",
    "        \"\"\"\n",
    "        # 1. Parse the phrase\n",
    "        self.parse_phrase()\n",
    "    \n",
    "        # 2. Vectorize the search phrase\n",
    "        self.query_vector = vectorizer.content_to_vector(self.phrase, self.vocabulary)\n",
    "    \n",
    "        # 3. Loop over nodes and score\n",
    "        scores = {}\n",
    "        for url, data in webgraph.graph.nodes(data=True):\n",
    "            vector = data.get(\"vector\")\n",
    "            if vector is None:\n",
    "                continue\n",
    "    \n",
    "            if not self.filter_vector(vector):\n",
    "                continue\n",
    "    \n",
    "            score = self.cosine_similarity(self.query_vector, vector)\n",
    "            if score > 0:\n",
    "                scores[url] = score\n",
    "    \n",
    "        if not scores:\n",
    "            return []\n",
    "    \n",
    "        # 4. Get pageranks\n",
    "        pageranks = {\n",
    "            url: data.get(\"pagerank\", 0.0)\n",
    "            for url, data in webgraph.graph.nodes(data=True)\n",
    "        }\n",
    "    \n",
    "        # 5. Get top-ranked URLs\n",
    "        top_urls = self.top_ranking(scores, pageranks)\n",
    "    \n",
    "        # 6. Build final result entries\n",
    "        results = []\n",
    "        for url in top_urls:\n",
    "            data = webgraph.graph.nodes[url]\n",
    "            title = data.get(\"title\", \"Untitled\")\n",
    "            content = data.get(\"content\", \"\")\n",
    "            similarity_score = scores.get(url, 0.0)\n",
    "            pagerank_score = pageranks.get(url, 0.0)\n",
    "            subtitle = self.generate_subtitle(content)\n",
    "    \n",
    "            results.append({\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"subtitle\": subtitle,\n",
    "                \"similarity_score\": round(similarity_score, 4),\n",
    "                \"pagerank_score\": round(pagerank_score, 4)\n",
    "            })\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def parse_phrase(self) -> tuple[set[str], set[str]]:\n",
    "        \"\"\"\n",
    "        Parse the search phrase into must-have and must-not-have word sets.\n",
    "    \n",
    "        Logic:\n",
    "            - Quoted words are split and added to must-have words.\n",
    "            - Dash-prefixed words go to must-not-have set.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: (must_words, must_not_words)\n",
    "        \"\"\"\n",
    "        must_words = set()\n",
    "        must_not_words = set()\n",
    "    \n",
    "        # Find quoted parts using regular expression library\n",
    "        quoted_phrases = re.findall(r'\"([^\"]+)\"', self.phrase)\n",
    "        for phrase in quoted_phrases:\n",
    "            must_words.update(phrase.lower().split())\n",
    "    \n",
    "        # Remove quoted parts from original string\n",
    "        cleaned = re.sub(r'\"[^\"]+\"', '', self.phrase)\n",
    "    \n",
    "        # Process remaining words\n",
    "        for word in cleaned.split():\n",
    "            word = word.strip().lower()\n",
    "            if word.startswith('-') and len(word) > 1:\n",
    "                must_not_words.add(word[1:])\n",
    "    \n",
    "        self.must_words = must_words\n",
    "        self.must_not_words = must_not_words\n",
    "        return must_words, must_not_words\n",
    "\n",
    "\n",
    "    def filter_vector(self, vector: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the given vector satisfies the must-have and must-not-have word rules.\n",
    "    \n",
    "        Args:\n",
    "            vector (np.ndarray): Vectorized content from a WebGraph node.\n",
    "    \n",
    "        Returns:\n",
    "            bool: True if eligible for similarity check, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.query_vector is None:\n",
    "            return False\n",
    "    \n",
    "        for word in self.must_words:\n",
    "            if word in self.vocabulary:\n",
    "                index = self.vocabulary.index(word)\n",
    "                if vector[index] == 0:\n",
    "                    return False\n",
    "    \n",
    "        for word in self.must_not_words:\n",
    "            if word in self.vocabulary:\n",
    "                index = self.vocabulary.index(word)\n",
    "                if vector[index] > 0:\n",
    "                    return False\n",
    "    \n",
    "        return True\n",
    "    \n",
    "\n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two vectors.\n",
    "    \n",
    "        Args:\n",
    "            vec1 (np.ndarray): Query vector.\n",
    "            vec2 (np.ndarray): Document vector.\n",
    "    \n",
    "        Returns:\n",
    "            float: Similarity score between 0.0 and 1.0\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "        if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "            return 0.0\n",
    "    \n",
    "        return float(dot_product / (norm_vec1 * norm_vec2))\n",
    "\n",
    "\n",
    "    def top_ranking(self, scores: dict[str, float], pageranks: dict[str, float]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Combine similarity score and pagerank, sort by final score, and return top 10 nodes.\n",
    "    \n",
    "        Args:\n",
    "            scores (dict[str, float]): URL to cosine similarity score.\n",
    "            pageranks (dict[str, float]): URL to pagerank score.\n",
    "    \n",
    "        Returns:\n",
    "            list[str]: Top result entries with url as key\n",
    "        \"\"\"\n",
    "        combined = {}\n",
    "    \n",
    "        for url, sim in scores.items():\n",
    "            pr = pageranks.get(url, 0.0)\n",
    "            # Score values are near to zero, by powering pagerank, it impact less than similarity.\n",
    "            combined[url] = sim * (pr ** self.pagerank_score_damping) \n",
    "    \n",
    "        # Sort by combined score descending, return top 10 URLs\n",
    "        ranked = sorted(combined.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [url for url, _ in ranked[:self.count_results]]\n",
    "        \n",
    "\n",
    "    def generate_subtitle(self, content: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract segments of content surrounding must_words and other search words, max 200 characters.\n",
    "    \n",
    "        Args:\n",
    "            content (str): Full content from the node.\n",
    "    \n",
    "        Returns:\n",
    "            str: Formatted preview text with \"...\" between segments.\n",
    "        \"\"\"\n",
    "        words = content.split()\n",
    "        blocks = []\n",
    "        seen = set()\n",
    "        char_limit = 200\n",
    "    \n",
    "        # Priority: must_words, then other search words\n",
    "        priority_terms = list(self.must_words) + [\n",
    "            w for w in self.phrase.lower().split() # other words\n",
    "            if w not in self.must_words and not w.startswith('-') and w not in self.must_not_words\n",
    "        ]\n",
    "    \n",
    "        for term in priority_terms:\n",
    "            for i, word in enumerate(words): # need to have index for words\n",
    "                if word.lower().strip(string.punctuation) == term and term not in seen:\n",
    "                    seen.add(term)\n",
    "                    start = max(i - 3, 0) # 3 words before the occurance of the term\n",
    "                    end = min(i + 6, len(words)) # 6 words after the occorance of the term\n",
    "                    block = ' '.join(words[start:end])\n",
    "                    blocks.append(block)\n",
    "                    break  # Only one block per term\n",
    "    \n",
    "            if sum(len(b) for b in blocks) > char_limit:\n",
    "                break\n",
    "    \n",
    "        return \"... \" + \"... \".join(blocks)[:char_limit] + (\"...\" if blocks else \"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6d8f605-7c9d-4df2-8a4c-6f580d265ae0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Search Results:\n",
      "\n",
      "1. Főoldal - Budapesti Corvinus Egyetem\n",
      "   https://www.uni-corvinus.hu\n",
      "   Similarity Score: 0.0765\n",
      "   PageRank Score: 0.0047\n",
      "   ... hírek Események Beszélgetés Dávid-Barrett Tamás oxfordi kutatóval könyve magyarországi... Események Beszélgetés Dávid-Barrett Tamás oxfordi kutatóval könyve magyarországi megjelenése... További hírek ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === User input ===\n",
    "phrase = input(\"Enter your search phrase: \").strip()\n",
    "\n",
    "# === Initialize Searching ===\n",
    "searcher = Searching(phrase, vectorizer.vocabulary)\n",
    "\n",
    "# === Run search ===\n",
    "results = searcher.handler(vectorizer, webgraph)\n",
    "\n",
    "# === Display results ===\n",
    "if not results:\n",
    "    print(\"No relevant results found.\")\n",
    "else:\n",
    "    print(\"\\nTop Search Results:\\n\")\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   {result['url']}\")\n",
    "        print(f\"   Similarity Score: {result['similarity_score']}\")\n",
    "        print(f\"   PageRank Score: {result['pagerank_score']}\")\n",
    "        print(f\"   {result['subtitle']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6e34c-bf62-4605-aabc-58bfdd309b77",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#BUILD\n",
    "class UIHelper:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Help UI system to have access to instances and do the search.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_context(v, g, m, p) -> None:\n",
    "        \"\"\"\n",
    "        Share the instances with Flask.\n",
    "        \"\"\"\n",
    "        global global_vectorizer, global_webgraph, global_max_results, global_pagerank_damping_factor\n",
    "        global_vectorizer = v\n",
    "        global_webgraph = g\n",
    "        global_max_results = m\n",
    "        global_pagerank_damping_factor = p\n",
    "\n",
    "\n",
    "    def search(phrase: str) -> list[dict[str, str, str, float, float]]:\n",
    "        \"\"\"\n",
    "        Search the phrase using Searching object.\n",
    "    \n",
    "        Args:\n",
    "            phrase (str): Search phrase.\n",
    "    \n",
    "        Returns:\n",
    "            list[dict]: List of results/\n",
    "        \"\"\"\n",
    "        searcher = Searching(phrase, global_vectorizer.vocabulary, global_max_results, global_pagerank_damping_factor)\n",
    "        results = searcher.handler(global_vectorizer, global_webgraph)\n",
    "\n",
    "        return [\n",
    "            (r['url'], r['title'], r['subtitle'], r['similarity_score'], r['pagerank_score'])\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    def get_website_meta() -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Get website title and favicon URL from the root url, available in the webgraph.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: [website_title, favicon_url] or empty list if not found.\n",
    "        \"\"\"\n",
    "        header = {\n",
    "            'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"\n",
    "        }\n",
    "\n",
    "        url = list(global_webgraph.graph.nodes)[0] # The root url\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5, headers=header)\n",
    "            if response.status_code != 200:\n",
    "                return \"\", \"\"\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Get <meta property=\"og:title\" content=\"...\">\n",
    "            og_title = soup.find(\"meta\", property=\"og:title\")\n",
    "            website_title = og_title.get(\"content\", \"\") if og_title else \"\"\n",
    "\n",
    "            # Get <link rel=\"icon\" href=\"...\"> (sometimes rel=\"shortcut icon\")\n",
    "            icon_link = soup.find(\"link\", rel=lambda val: val and \"icon\" in val)\n",
    "            favicon = icon_link.get(\"href\", \"\") if icon_link else \"\"\n",
    "\n",
    "            return website_title.strip(), favicon.strip()\n",
    "\n",
    "        except requests.RequestException:\n",
    "            return \"\", \"\"\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
